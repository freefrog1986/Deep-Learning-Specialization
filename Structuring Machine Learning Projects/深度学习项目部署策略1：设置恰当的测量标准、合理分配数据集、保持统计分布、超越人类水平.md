# 机器学习项目结构化1：评估指标、分配数据集、关注统计分布、理解和超越人类水平、改善模型性能率等

## 概要
在搭建成功的机器学习项目过程中如何把握正确的工作方向意义重大！作为项目负责人，其中一项最重要的工作是保证团队的工作方向没有‘偏航’。这其中的关键方法就是应用正确的机器学习策略，其中包括：评估指标、分配数据集、关注统计分布、理解和超越人类水平、改善模型性能率等。当然，理论与实践同等重要，理解和选择这些策略视实际任务情况而定。本文主要包括两部分内容，第一部分是关于机器学习策略的理论分析。第二部分的内容主要是一个关于识别鸟类的案例实践。   

本文的主要内容来源于MOOC网站Coursera上的课程[course Structuring Machine Learning](https://www.coursera.org/learn/machine-learning-projects/home/welcome).

## 第一部分 机器学习策略
### 1. 为什么要使用机器学习策略
原因很简单直接，因为在实际部署机器学习项目过程中，有太多需要调整的参数和策略，例如何时该收集更多的数据、何时使用更深层的神经网络、使用Adam而不是梯度下降、采用Dropout、增加L2正则化、更换代价函数等等。
因此，学习在何种情况下采用何种策略能够大大节省时间。尤其当我们利用深度学习处理海量数据时，训练模型的时间往往需要数周甚至数月，如果加上收集数据和调整参数的时间，一个项目从出生到成功部署必须经历一个很长的周期才能完成，在这个漫长的项目周期中选择好的策略就显得尤为重要，帮助我们少走弯路，高效达成目标。

### 2. 正交化
正交化是我们在现实中经常使用的一个简单有效的方法，它的核心思想如下，当我们调整一个参数或应用一种新的改进方法时，仅影响模型的一个我们关心的指标，而不会对其他指标产生影响。   
这样做的好处是我们可以专心于改善模型的某一环节性能，当该方面性能得到改善后再关心其他环节，由于我们使用的改进方法是相互独立互不影响的，因此使调整模型的过程更加直观，节省时间。   
更具体来说，一般按顺序改善模型的以下4个环节：

1. 改善模型在训练集的表现
2. 改善模型在开发集的表现
3. 改善模型在测试集的表现
4. 改善模型在实际应用中的表现

举例来说，首先，我们搭建一个初步模型，该模型在训练集的表现是51%的准确率，这显然不满足我们的要求。此时我们关心的指标是准确率，所以应该采用策略提高准确率，例如采用更深层的神经网络。采用策略后，准确率提升到了99%，已经满足要求。但是当在开发集测试模型时，表现却很差（例如80%），此时我们主要关心的指标已经不再是准确率，而是通用性，因此我们采用正则化的策略，应用策略后，我们的模型在训练集和开发集的表现分别为95%和94%，Bingo！这就是一个简单的采用正交化思维改善模型性能的例子，避免重复工作，又高效达成指标！   
糟糕的策略是，一开始就关心准确率和通用性两个指标，希望采用的策略能同时提高准确率和通用性，往往导致得到的模型在训练集和开发集的表现都不理想。

### 3. 单一数字评估指标
使用单一数字评估指标作为模型性能优劣的评判标准，使我们的工作目标更加明确。   
我们来看下面这个例子：
识别猫的数据集中，我们用1来标记一张图片有猫，用0来标记一张图片中没有猫。
当图片经过模型的识别，得到下表：
![classifier](https://github.com/freefrog1986/markdown/blob/master/classifier.jpeg?raw=true)

由上表可以得到两个比较重要的评估指标：查准率（Precision）和查全率(Recal),公式如下：
$$Precision(\%) = \frac{True positive}{(True positive+False positive)} \times 100$$
$$Recall(\%) = \frac{True positive}{(True negative+False negative)} \times 100$$
假设我们的两个模型A和B得到如下结果：

Classifier|Precision(p)|Recall(r)
-|-|-
A|95%|90%|
B|98%|85%|

哪一个模型更优呢？这是一个很难回答的问题。
如果只用单一数字评估指标的话，问题会变得简单的多！
我么采用F1-score作为单一数字评估指标，它的公式如下：
$$F1-Score = \frac{2}{\frac{1}{p}+\frac{1}{r}}$$

利用公式，我们得到：

Classifier|Precision(p)|Recall(r)|F1-Score
-|-|-|-
A|95%|90%|92.4%
B|98%|85%|91.9%

目标变得明确了！！！我们的目标从关注两个指标变成关注一个单一指标！从而轻而易举的比较不同模型之间性能优劣！

### 4. 满足和优化指标
实际任务中，可能不止上面讨论的单一数字评估指标，想象一种情况，我们模型的性能达到F1-Score为92.4%，运行时间为1500ms，另一个模型的F1-Score为91%，运行时间为15ms。这两种模型哪种更优呢？
显然，如果考虑到实际应用中，对运行速度有要求，显然第二种模型更优，即使它牺牲了一点精度。   
一般来说，可以将评估指标分为两类，满足指标和优化指标。   
- 满足指标相当于给模型性能设定的阈值，例如运行时间不能超过20ms。   
- 优化指标就是在符合满足指标的前提下，我们希望模型做到尽可能的好，例如上面提到的F1-Score就是优化指标。另外，为便于评估性能，优化指标一般只有一个。

### 5. 训练、开发、测试集统计分布
如何选择训练、开发和测试数据集至关重要，其中重要的两点原则包括：
1. 保持开发和测试集的统计分布一致。
2. 随机从所有数据中选取数据组成开发和测试集。

### 6. 开发和测试集的大小
原来的数据集分配方法一般是7-3分配，或6-2-2分配。但是随着数据量的增大，一般来说我们只把很少的一部分数据作为开发和测试集即可，例如按照98-1-1的比例分配。

### 7. 改变开发和测试集指标
以识别猫的图片来说明：
假设我们算法优化指标是图片识别的错误率，我们的两个算法达到的错误率如下：

Algorithm| Classification error[%]
-|-
A|3%
B|5%

我们定义错误率如下：
$$Error = \frac{1}{m_{dev}}\sum_{i=1}^{m_{dev}}L\{\hat y^{i} \neq y^{i}\}$$

根据要求，项目组放弃了错误率较低的A算法，而选用了错误率较高的B算法，原因是B算法没有错误的将色情图片归类为猫图片。为了提高A算法的性能，我们需要该表我们的评估指标。
对于这个例子来说，我们需要在识别图片过程中加入对色情图片的判断，因此我们在原来的评估指标中加入一个权值w，w满足如下条件：
$$w^{i}\begin{cases}
1 & \text{ if } x^{i}\ \text{is non-pornographic} \\ 
100 & \text{ if } x^{i}\ \text{is pornographic}
\end{cases}$$

那么我们的错误率公式改进如下：
$$Error = \frac{1}{\sum w^{i}}\sum_{i=1}^{m_{dev}}w^{i}L\{\hat y^{i} \neq y^{i}\}$$

也就是说，当我们发现原始的评估指标不符合我们真实的目标时，应改善或重新定义优化指标作为评估指标。

### 8. 人类水平
为什么很多项目将达到或超越‘人类水平’作为目标？   
因为在很多领域，人类达到了相当高的水平，例如图像识别。可以说在这些人类非常擅长的领域，‘人类水平’的表现几乎是完美的。因此，常常利用‘人类水平’近似作为该领域的‘完美表现’。实际上，真实的‘完美表现’我们定义为'Bayes optimal'。   
也就是说'Bayes optimal'代表无论是人还是机器都无法超越，只能无限接近的‘完美表现’。   
由于'Bayes optimal'很难定义，因此在很多领域，我们用‘人类水平’近似代表'Bayes optimal'   

### 9. 可避免偏差
定义了人类水平作为参考标准，我们就可以利用这个工具指导算法改进的方向了！   
首先，先给出理论基础，我们定义训练集错误率与目标指标（这里是人类水平）之间的差距为‘bias’。而训练集与开发集之间的差距定义为‘Vriance’。如果bias大于Vriance，那么我们就采用减少bias的策略，反之则采用减少Variance的策略。   
举例如下：

-| Classification error[%]
-|-|-
-|Scenario A|Scenario B-
Humans|1|7.5
Training error|8|8
Dvelopment error|10|10

在上面的例子中，我们考虑两种情况下该选择何种策略改进算法。   
**Scenario A**
A情况下，人类表现的错误率为1%，训练集的错误率是8%，这其中的差距bias是7%。开发集的错误率是10%，与训练集的差距variance是2%。因此我们应采用减少bias的策略，如增加神经网路的深度或增加网络的迭代次数。   
**Scenario B**
B情况下，bias为0.5，而variance为2，因此我们采用减少Variance的策略如正则化或增加训练集大小。   
### 10. 理解人类水平
看来用‘人类水平’作为‘Bayes optiml’的近似表示对于把握算法改进的方向具有很大意义。接下来要讨论的问题是：如何理解‘人类水平‘呢？   
先从一个问题开始说起：对于一项‘诊断医疗图像’的任务来说，假设我们经过调研，得到了下面的一组结果，那么我们应该选择哪个指标作为‘人类水平’呢？   

-|classification error(%)
-|-
Typical Human|3.0
Typical Doctor|1.0
Experienced Doctor|0.7
Team of Experienced Doctors|0.5

在没有其他额外条件的情况下，我们应该以**人类能够达到的最高水平**作为‘人类水平’，因为‘人类水平’作为完美而无法超越的‘Bayes optimal’的近似，约接近这个完美值（这个案例中是0%），约适合作为它的近似。

### 11. 超越人类水平
以下面的任务为例：

-|classification error(%)
-|-|-
-|Scenario A |Scenario B
Team of Humans|0.5|0.5
One Human|1.0|1.0
Training error|0.6|0.3
Development error|0.8|0.6

Scenario A中算法的表现还没有超越‘人类水平’，我们仍然有改进空间。
而当达到Scenario B中的算法性能，恭喜你！你已经超越了人类水平，这时候，传统的改善bias或改善variance的方法都不在适用了！

### 12. 改善模型性能
总的来说，我们有两个基础的改善监督学习的策略，一个是改善bias，另一个是改善variance.
判断条件是，若训练误差与人类水平差距太大，则采用改善bias的策略，若开发集与训练集差距太大，则采用改善variance的策略。总结如下图：
![bias and variance](https://github.com/freefrog1986/markdown/blob/master/bias%20and%20variance.jpeg?raw=true)
## 第二部分 案例研究：Bird recognition in the city of Peacetopia

How to set right direction in the process of building up a successful machine Learning Projects is significant. As a leader of project, the main task is to make sure your team aren't moving away from your goals. The key method is adopt appropriate strategies including setting metrics, structuring your data, considering dataset distribution, choosing optimal methods, defining right human-level performance, Speeding up your work etc. Making decicions of those methods based on actual conditions.   

The following content actually are case study about recognition of birds in city.   

This case study is origenally from a test in coursera. You can find it in course [Structuring Machine Learning Projects](https://www.coursera.org/learn/machine-learning-projects/home/welcome).

### 1. Problem Statement

This example is adapted from a real production application, but with details disguised to protect confidentiality.

You are a famous researcher in the City of Peacetopia. The people of Peacetopia have a common characteristic: they are afraid of birds. To save them, you have **to build an algorithm that will detect any bird flying over Peacetopia and alert the population**.

The City Council gives you a dataset of 10,000,000 images of the sky above Peacetopia, taken from the city’s security cameras. They are labelled:

- y = 0: There is no bird on the image
- y = 1: There is a bird on the image
Your goal is to build an algorithm able to classify new images taken by security cameras from Peacetopia.

There are a lot of decisions to make:

- What is the evaluation metric?
- How do you structure your data into train/dev/test sets?

**Metric of success**

The City Council tells you the following that they want an algorithm that

Has high accuracy
Runs quickly and takes only a short time to classify a new image.
Can fit in a small amount of memory, so that it can run in a small processor that the city will attach to many different security cameras.   
Note: Having three evaluation metrics makes it harder for you to quickly choose between two different algorithms, and will slow down the speed with which your team can iterate. True/False?

@True   
False

### 2. Choosing Model
After further discussions, the city narrows down its criteria to:
"We need an algorithm that can let us know a bird is flying over Peacetopia as accurately as possible."
"We want the trained model to take no more than 10sec to classify a new image.”
“We want the model to fit in 10MB of memory.”
If you had the three following models, which one would you choose?   


index| Test Accuracy | Runtime | Memory size
----|----|------|----
model A|97% | 1 sec  | 3MB
model B|99% |13 sec |9MB
model C|97% |3 sec  |2MB
model D|98% |9 sec  |9MB

Accuracy is an optimizing metric; running time and memory size are a satisficing metrics.
Satisficing metric make us drop model B, In the remaining options, model D proformance best at Test Accuracy. So D is a better choice.

### 3. Structuring your data

Before implementing your algorithm, you need to split your data into train/dev/test sets. Which of these do you think is the best choice?

option|Train    |Dev    |Test
----|----|----|----|----
A|6,000,000 |3,000,000  |1,000,000
B|3,333,334 |3,333,333  |3,333,333
C|9,500,000 |250,000    |250,000
D|6,000,000  |1,000,000 |3,000,000

For big data, espcially more than 1,000,000, we should use a big part to train our model and leave a small part to develop and test. So C is a better choice.

### 4. Change training set distribution

After setting up your train/dev/test sets, the City Council comes across another 1,000,000 images, called the “citizens’ data”. Apparently the citizens of Peacetopia are so scared of birds that they volunteered to take pictures of the sky and label them, thus contributing these additional 1,000,000 images. These images are different from the distribution of images the City Council had originally given you, but you think it could help your algorithm.

You should not add the citizens’ data to the training set, because this will cause the training and dev/test set distributions to become different, thus hurting dev and test set performance. 

@False

Adding this data to the training set will change the training set distribution. However, it is not a problem to have different training and dev distribution. On the contrary, it would be very problematic to have different dev and test set distributions.

### 5. Change testing set distribution

One member of the City Council knows a little about machine learning, and thinks you should add the 1,000,000 citizens’ data images to the test set. You object because:
- The test set no longer reflects the distribution of data (security cameras) you most care about.
- This would cause the dev and test set distributions to become different. This is a bad idea because you’re not aiming where you want to hit.

### 6. Next move

You train a system, and its errors are as follows (error = 100%-Accuracy):

Training set error|4.0%
----|----
Dev set error|4.5%

This suggests that one good avenue for improving performance is to train a bigger network so as to drive down the 4.0% training error. Do you agree?   
No, because there is insufficient information to tell.

### 7. human-level performance

You ask a few people to label the dataset so as to find out what is human-level performance. You find the following levels of accuracy:

Bird watching expert #1|    0.3% error
-|-
Bird watching expert #2|    0.5% error
Normal person #1 (not a bird watching expert)|  1.0% error
Normal person #2 (not a bird watching expert)|  1.2% error

If your goal is to have “human-level performance” be a proxy (or estimate) for Bayes error, how would you define “human-level performance”?   
0.3% (accuracy of expert #1) is your best choice, cause someone can achive 0.3% error means that the Bayes error is beter than 0.3.

### 8.Bayes level 

Which of the following statements do you agree with?
1. A learning algorithm’s performance can be better human-level performance but it can never be better than Bayes error.
2. A learning algorithm’s performance can never be better human-level performance but it can be better than Bayes error.
3. A learning algorithm’s performance can never be better than human-level performance nor better than Bayes error.
4. A learning algorithm’s performance can be better than human-level performance and better than Bayes error.   

First statment is correct.

### 9. Optimaze strategy for bias

You find that a team of ornithologists debating and discussing an image gets an even better 0.1% performance, so you define that as “human-level performance.” After working further on your algorithm, you end up with the following:

Human-level performance|0.1%
-|-
Training set error| 2.0%
Dev set error|  2.1%

Based on the evidence you have, which two of the following four options seem the most promising to try? (Check two options.)
- [x] Try decreasing regularization.
- [ ] Try increasing regularization. 
- [x] Train a bigger model to try to do better on the training set.
- [ ] Get a bigger training set to reduce variance.

### 10. Optimaze strategy for overfit

You also evaluate your model on the test set, and find the following:

Human-level performance|    0.1%
-|-
Training set error| 2.0%
Dev set error|  2.1%
Test set error| 7.0%

What does this mean? (Check the two best options.)
- [] You have underfit to the dev set.
- [x] You have overfit to the dev set.
- [] You should get a bigger test set.
- [x] You should try to get a bigger dev set.

### 11. Surpass humman level 

After working on this project for a year, you finally achieve:
What can you conclude? (Check all that apply.)

- [] This is a statistical anomaly (or must be the result of statistical noise) since it should not be possible to surpass human-level performance.
- [] With only 0.09% further progress to make, you should quickly be able to close the remaining gap to 0%
- [x] It is now harder to measure avoidable bias, thus progress will be slower going forward.
- [x] If the test set is big enough for the 0,05% error estimate to be accurate, this implies Bayes error is ≤0.05

### 12. Set appropriate metric

It turns out Peacetopia has hired one of your competitors to build a system as well. Your system and your competitor both deliver systems with about the same running time and memory size. However, your system has higher accuracy! However, when Peacetopia tries out your and your competitor’s systems, they conclude they actually like your competitor’s system better, because even though you have higher overall accuracy, you have more false negatives (failing to raise an alarm when a bird is in the air). What should you do?

- [] Look at all the models you’ve developed during the development process and find the one with the lowest false negative error rate.
- [] Ask your team to take into account both accuracy and false negative rate during development.
- [x] Rethink the appropriate metric for this task, and ask your team to tune to the new metric.
- [] Pick false negative rate as the new metric, and use this new metric to drive all further development.

### 13. Adding new data

You’ve handily beaten your competitor, and your system is now deployed in Peacetopia and is protecting the citizens from birds! But over the last few months, a new species of bird has been slowly migrating into the area, so the performance of your system slowly degrades because your data is being tested on a new type of data.


You have only 1,000 images of the new species of bird. The city expects a better system from you within the next 3 months. Which of these should you do first?

- [x] Use the data you have to define a new evaluation metric (using a new dev/test set) taking into account the new species, and use that to drive further progress for your team.
- [] Put the 1,000 images into the training set so as to try to do better on these birds.
- [] Try data augmentation/data synthesis to get more images of the new type of bird.
- [] Add the 1,000 images into your dataset and reshuffle into a new train/dev/test split.

### 14. Speed up your work

The City Council thinks that having more Cats in the city would help scare off birds. They are so happy with your work on the Bird detector that they also hire you to build a Cat detector. (Wow Cat detectors are just incredibly useful aren’t they.) Because of years of working on Cat detectors, you have such a huge dataset of 100,000,000 cat images that training on this data takes about two weeks. Which of the statements do you agree with? (Check all that agree.)

- [x] Buying faster computers could speed up your teams’ iteration speed and thus your team’s productivity.
- [x] If 100,000,000 examples is enough to build a good enough Cat detector, you might be better of training with just 10,000,000 examples to gain a ≈10x improvement in how quickly you can run experiments, even if each model performs a bit worse because it’s trained on less data.
- [] Having built a good Bird detector, you should be able to take the same model and hyperparameters and just apply it to the Cat dataset, so there is no need to iterate.
- [x] Needing two weeks to train will limit the speed at which you can iterate.


