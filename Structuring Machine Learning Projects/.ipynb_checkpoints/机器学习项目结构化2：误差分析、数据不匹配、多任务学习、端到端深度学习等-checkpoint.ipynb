{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习项目结构化2：误差分析、数据不匹配、多任务学习、端到端深度学习等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该文章是机器学习项目结构化的第二部分内容，主要讨论在部署机器学习项目的过程中遇到的各类问题，以及解决思路。文章结构分为两部分，第一部分是机器学习策略的理论分析和探讨，第二部分是案例分析，通过模拟实际案例理解策略在真实项目中的应用。第一部分具体来说提出了机器学习项目的常见问题：如何判断改进算法的方向，如何处理错误标记的数据，如何处理不匹配的数据集，如何处理面数据不足的任务，如何同时处理多个任务。针对上述问题，第一部分讨论了误差分析、解决数据不匹配、迁移和多任务学习、端到端学习等工作策略。\n",
    "本文的主要内容来源于吴恩达在MOOC网站Coursera上发布的深度学习系列课程，[网址点这里](https://www.coursera.org/learn/machine-learning-projects/home/week/2)。本文是该课程的学习笔记。直接去学习该课程是理解本文知识的最好方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分 机器学习策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 误差分析（Error analysis）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**如何做**   \n",
    "误差分析就是对开发集中被我们的模型错误分类的数据进行分析，一般来说，手动分析是有效的方式。   \n",
    "误差分析帮助我们判断，采取什么样的策略能够帮助进一步提高模型的准确率。   \n",
    "例如在‘识别猫的图片’任务中，假设我们有100张被错误分类的图片，那么手动将这100张图片标注为‘错误的将狗识别为猫’、‘将猫科动物识别为猫’、‘图片模糊不清’的其中一种，然后计算其中各类原因导致错误分类的占比，例如30%为‘错误的将狗识别为猫’，那么改进算法对狗的识别率将能够降低算法最多30%的错误率。   \n",
    "\n",
    "**错误标记的数据**   \n",
    "在做误差分析的过程中，有可能遇到这样的情况：有些图片被标注错了，例如将有猫的图片标注为0。遇到这样的情况该如何处理？是否应该将全部被错误标记的数据找出来并纠正？下面我们来讨论这个问题。 \n",
    "1. 如果标记错误出现在训练集中，被错误标记的数据是随机误差，那么可以不用纠正，因为深度学习算法对此类错误具有很强的鲁棒性。\n",
    "2. 如果标记错误出现在训练集中，被错误标记的数据不是随机误差，例如将所有白色的狗都标记成了猫，那么，为避免深度学习对该类型的图片形成错误判断，需要对其进行纠正。\n",
    "3. 如果担心错误标记的数据出现在开发或测试集，应进行误差分析，统计错误标记的数据占比。\n",
    "4. 如果在开发或测试集中的错误标记的数据严重影响了算法性能，那么应该纠正，否则可以不用纠正。   \n",
    "\n",
    "一旦决定了对错误标记的数据进行纠正，以下是几条原则：   \n",
    "- 对开发和测试集采取同样的处理，以保证开发和测试集来自同一分布。\n",
    "- 不仅要考虑被算法错误分类的数据，也要考虑被算法正确分类的数据，因为有可能只是因为运气好，才把被错误标记的数据判断正确。\n",
    "- 大部分时候不用对训练集进行纠正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 不匹配的训练和开发\\测试集（mismatched training and dev/test set）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**什么是数据不匹配**   \n",
    "举例说明，我们的任务是识别猫的图片，我们的训练集是清晰的、专业拍摄的猫的图片，但是我们需要将算法部署在手机端，而开发集和测试集的猫的图片是模糊的、非专业拍摄的，这就造成了训练和开发\\测试集的数据不匹配，简单来说，就是来自不同分布。   \n",
    "\n",
    "**训练-开发数据集**   \n",
    "考虑以下情况，人类水平是0%，训练集误差是1%，开发集误差是10%，我们能够得到什么结论？算法的泛化性差？  \n",
    "如果不考虑其他特殊情况，确实是算法的泛化性差，需要采取减少variance的策略，但是如果考虑数据的不匹配呢？   \n",
    "如果考虑数据的不匹配，很难得出确定的答案，是算法泛化性差，还是因为在开发集的数据与训练集数据不匹配。   \n",
    "为了分析导致原因，我们引入‘训练-开发数据集’。‘训练-开发数据集’由训练集的一小部分组成。现在我们多了一个误差，就是‘训练-开发数据集’误差，下面考虑两种情况：\n",
    "1. ‘训练-开发数据集’误差是9%，那么可以判断此时是泛华性问题，因为来自同一分布的数据出现了8%的误差差距，来自不同分布的数据只有1%的误差差距。\n",
    "2. ‘训练-开发数据集’误差是1.5%，那么可以判断此时是数据不匹配问题，因为来自同一分布的数据只有0.5%的误差差距，而来自不同分布的数据有8.5%的误差差距。\n",
    "\n",
    "**解决数据不匹配问题**   \n",
    "如何解决数据不匹配问题？   \n",
    "1. 利用误差分析，理解训练集、开发集和测试集的不同。\n",
    "2. 使训练集的数据和开发/测试集的数据更加相似。例如采用数据合成或收集更相似的数据。\n",
    "数据合成的问题，有些情况下，采用合成的数据只是真实世界中该类数据的很小的子集，这样容易导致算法只对这一类数据起作用。因此合成时应尽量使用复杂的多样性的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 多任务学习 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**迁移学习**   \n",
    "当我们想处理一类新的任务，但是有没有足够多的数据时，可以使用‘迁移学习’！\n",
    "例如，我们想对‘医学诊断图片’进行分类，但是只有1000张图片，而我们拥有猫的图片1000000张，此时可以将用于训练识别猫的神经网络迁移到识别医学图片，我们需要做的是，保持网络中的前n层权重不变，金改变最后的一层或几层的激活函数，然后利用新的医学图片进行训练即可。   \n",
    "迁移学习背后的理论基础是，对于同一类型的任务，深度学习的前几层网络负责处理一些基础的工作，这些工作是通用的，可迁移的。对于图片识别来说，前几层用于识别图片的一些线条，轮廓之类的，而医学图片识别的任务同样需要做这些工作，因此，直接将前几层已经训练好的网络直接迁移过来使用是行得通的。   \n",
    "\n",
    "**什么情况下使用迁移学习**   \n",
    "- 当任务A和任务B有图样的输入时，例如上述例子中输入都是图片。\n",
    "- 当我们有更多的数据关于任务A时\n",
    "- 从任务A得到的一些低级别的特征对于任务B有帮助时。\n",
    "\n",
    "**什么是多任务学习**\n",
    "举例说明，无人驾驶汽车的摄像头收集的图片中，我们需要同时识别交通标志、行人、其他车辆等多个目标，而不是我们之前一直处理的单一目标任务，这时就需要用到多任务学习。\n",
    "多任务学习的原理很简单，将输出y标记为向量，每一个元素代表一个任务的类别，例如，用$y = \\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\0 \\end{bmatrix}$ 表示4类目标的类别，第一类是是否有行人，0代表没有。第二类是是否有其他车辆，1代表有。第三类是是否有停止标志，1代表有。第四类是是否有交通信号灯，0代表没有。\n",
    "如何定义代价函数呢，一个简单的方法是对四个类别分别求代价然后取平均。\n",
    "这里的一个小tips是，如果有些标签没有任何标注，也就是none或？，例如$y = \\begin{bmatrix} 0\\\\ ？\\\\ 1\\\\？ \\end{bmatrix}$这种情况，只需要忽略该标签对应的类别即可。\n",
    "\n",
    "**什么情况下使用多任务学习**\n",
    "- 多个任务能够共享低级别的特征。\n",
    "- 多个任务使用的数据非常相似。\n",
    "- 一个足够大的神经网路能够在所有任务上做的很好。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.端到端深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**什么是端到端学习**   \n",
    "端到端学习是一种解决问题的思路，与之对应的是多步骤解决问题，也就是将一个问题拆分为多个步骤分步解决，而端到端是由输入端的数据直接得到输出端的结果。但是一般情况下，更常见的是介于两者之间的解决思路，先把任务拆解为简单的两步解决。例如人脸识别门禁系统，不是由检测到的图片直接得到结果（识别出该员工的id），而是第一步将检测到的图片中的人脸部分放大居中，第二步通过识别得到该员工的id。\n",
    "\n",
    "**端到端学习的优劣**   \n",
    "优点：   \n",
    "- 让数据自己找到解决办法，而不是认为设定思路和步骤。\n",
    "- 需要很少的人工设计的部分，工作量大大减少。\n",
    "缺点：   \n",
    "- 需要大量数据。\n",
    "- 有可能排除了很有用的人工设计的部分。\n",
    "\n",
    "**什么情况下使用端到端的方法**   \n",
    "关键问题是，你是否有支撑端到端学习的足够多的数据，如果有的话，可以使用。没有的话可以选择更常用的两步法，第一步将任务分解为两个容易解决的子任务，第二步，分别解决。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分 案例研究：Autonomous driving (case study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. First step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you practice strategies for machine learning, in this week we’ll present another scenario and ask how you would act. We think this “simulator” of working in a machine learning project will give a task of what leading a machine learning project could be like!\n",
    "\n",
    "You are employed by a startup building self-driving cars. You are in charge of detecting road signs (stop sign, pedestrian crossing sign, construction ahead sign) and traffic signals (red and green lights) in images. The goal is to recognize which of these objects appear in each image. As an example, the above image contains a pedestrian crossing sign and red traffic lights\n",
    "\n",
    "\n",
    "Your 100,000 labeled images are taken using the front-facing camera of your car. This is also the distribution of data you care most about doing well on. You think you might be able to get a much larger dataset off the internet, that could be helpful for training even if the distribution of internet data is not the same.\n",
    "\n",
    "You are just getting started on this project. What is the first thing you do? Assume each of the steps below would take about an equal amount of time (a few days).\n",
    "\n",
    "-[x] Spend a few days training a basic model and see what mistakes it makes.\n",
    "\n",
    "-[] Spend a few days getting the internet data, so that you understand better what data is available.\n",
    "\n",
    "-[] Spend a few days collecting more data using the front-facing camera of your car, to better understand how much data per unit time you can collect.\n",
    "\n",
    "-[] Spend a few days checking what is human-level performance for these tasks so that you can get an accurate estimate of Bayes error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Output layer activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to detect road signs (stop sign, pedestrian crossing sign, construction ahead sign) and traffic signals (red and green lights) in images. The goal is to recognize which of these objects appear in each image. You plan to use a deep neural network with ReLU units in the hidden layers.\n",
    "\n",
    "For the output layer, a softmax activation would be a good choice for the output layer because this is a multi-task learning problem. True/False?\n",
    "\n",
    "-[] True\n",
    "\n",
    "-[x] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are carrying out error analysis and counting up what errors the algorithm makes. Which of these datasets do you think you should manually go through and carefully examine, one image at a time?\n",
    "\n",
    "-[] 10,000 images on which the algorithm made a mistake\n",
    "\n",
    "-[] 500 randomly chosen images\n",
    "\n",
    "-[x] 500 images on which the algorithm made a mistake\n",
    "\n",
    "-[] 10,000 randomly chosen images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Missing value label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working on the data for several weeks, your team ends up with the following data:\n",
    "\n",
    "100,000 labeled images taken using the front-facing camera of your car.\n",
    "900,000 labeled images of roads downloaded from the internet.\n",
    "Each image’s labels precisely indicate the presence of any specific road signs and traffic signals or combinations of them. For example, $y^{(i)} = \\begin{bmatrix}1\\\\ 0\\\\ 0\\\\ 1\\\\ 0\\\\ \\end{bmatrix}$ means the image contains a stop sign and a red traffic light.\n",
    "Because this is a multi-task learning problem, you need to have all your y(i) vectors fully labeled. If one example is equal to $y^{(i)} = \\begin{bmatrix}0\\\\ ?\\\\ 1\\\\ 1\\\\ ?\\\\ \\end{bmatrix}$ then the learning algorithm will not be able to use that example. True/False?   \n",
    "False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of data you care about contains images from your car’s front-facing camera; which comes from a different distribution than the images you were able to find and download off the internet. How should you split the dataset into train/dev/test sets?\n",
    "-[x] Choose the training set to be the 900,000 images from the internet along with 80,000 images from your car’s front-facing camera. The 20,000 remaining images will be split equally in dev and test sets.\n",
    "\n",
    "-[] Mix all the 100,000 images with the 900,000 images you found online. Shuffle everything. Split the 1,000,000 images dataset into 980,000 for the training set, 10,000 for the dev set and 10,000 for the test set.\n",
    "\n",
    "-[] Mix all the 100,000 images with the 900,000 images you found online. Shuffle everything. Split the 1,000,000 images dataset into 600,000 for the training set, 200,000 for the dev set and 200,000 for the test set.\n",
    "\n",
    "-[] Choose the training set to be the 900,000 images from the internet along with 20,000 images from your car’s front-facing camera. The 80,000 remaining images will be split equally in dev and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you’ve finally chosen the following split between of the data:\n",
    "\n",
    "Dataset:|    Contains:|   Error of the algorithm:\n",
    "-|-|-\n",
    "Training|    940,000 images randomly picked from (900,000 internet images + 60,000 car’s front-facing camera images)| 8.8%\n",
    "Training-Dev|    20,000 images randomly picked from (900,000 internet images + 60,000 car’s front-facing camera images)|  9.1%\n",
    "Dev| 20,000 images from your car’s front-facing camera|   14.3%\n",
    "Test|    20,000 images from the car’s front-facing camera|    14.8%\n",
    "\n",
    "You also know that human-level error on the road sign and traffic signals classification task is around 0.5%. Which of the following are True? (Check all that apply).\n",
    "\n",
    "-[] You have a large variance problem because your training error is quite higher than the human-level error.\n",
    "\n",
    "-[] Your algorithm overfits the dev set because the error of the dev and test sets are very close.\n",
    "\n",
    "-[x] You have a large avoidable-bias problem because your training error is quite a bit higher than the human-level error.\n",
    "\n",
    "-[x] You have a large data-mismatch problem because your model does a lot better on the training-dev set than on the dev set\n",
    "\n",
    "-[] You have a large variance problem because your model is not generalizing well to data from the same training distribution but that it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Different distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on table from the previous question, a friend thinks that the training data distribution is much easier than the dev/test distribution. What do you think?\n",
    "\n",
    "-[] Your friend is right. (I.e., Bayes error for the training data distribution is probably lower than for the dev/test distribution.)\n",
    "\n",
    "-[] Your friend is wrong. (I.e., Bayes error for the training data distribution is probably higher than for the dev/test distribution.)\n",
    "\n",
    "-[x] There’s insufficient information to tell if your friend is right or wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Manually check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You decide to focus on the dev set and check by hand what are the errors due to. Here is a table summarizing your discoveries:\n",
    "\n",
    "Overall dev set error|   14.3%\n",
    "-|-\n",
    "Errors due to incorrectly labeled data|  4.1%\n",
    "Errors due to foggy pictures|    8.0%\n",
    "Errors due to rain drops stuck on your car’s front-facing camera|    2.2%\n",
    "Errors due to other causes|  1.0%\n",
    "\n",
    "In this table, 4.1%, 8.0%, etc.are a fraction of the total dev set (not just examples your algorithm mislabeled). I.e. about 8.0/14.3 = 56% of your errors are due to foggy pictures.\n",
    "\n",
    "The results from this analysis implies that the team’s highest priority should be to bring more foggy pictures into the training set so as to address the 8.0% of errors in that category. True/False?\n",
    "\n",
    "-[] True because it is the largest category of errors. As discussed in lecture, we should prioritize the largest category of error to avoid wasting the team’s time.\n",
    "\n",
    "-[] True because it is greater than the other error categories added together (8.0 > 4.1+2.2+1.0).\n",
    "\n",
    "-[x] False because this would depend on how easy it is to add this data and how much you think your team thinks it’ll help.\n",
    "\n",
    "-[] False because data augmentation (synthesizing foggy images by clean/non-foggy images) is more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Degree of effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can buy a specially designed windshield wiper that help wipe off some of the raindrops on the front-facing camera. Based on the table from the previous question, which of the following statements do you agree with?\n",
    "\n",
    "-[x] 2.2% would be a reasonable estimate of the maximum amount this windshield wiper could improve performance.\n",
    "\n",
    "-[] 2.2% would be a reasonable estimate of the minimum amount this windshield wiper could improve performance.\n",
    "\n",
    "-[] 2.2% would be a reasonable estimate of how much this windshield wiper will improve performance.\n",
    "\n",
    "-[] 2.2% would be a reasonable estimate of how much this windshield wiper could worsen performance in the worst case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Synthesized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You decide to use data augmentation to address foggy images. You find 1,000 pictures of fog off the internet, and “add” them to clean images to synthesize foggy days, like this:\n",
    "\n",
    "\n",
    "Which of the following statements do you agree with? (Check all that apply.)\n",
    "\n",
    "-[x] So long as the synthesized fog looks realistic to the human eye, you can be confident that the synthesized data is accurately capturing the distribution of real foggy images, since human vision is very accurate for the problem you’re solving.\n",
    "\n",
    "-[] There is little risk of overfitting to the 1,000 pictures of fog so long as you are combing it with a much larger (>>1,000) of clean/non-foggy images.\n",
    "\n",
    "-[] Adding synthesized images that look like real foggy pictures taken from the front-facing camera of your car to training dataset won’t help the model improve because it will introduce avoidable-bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Incorrectly labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working further on the problem, you’ve decided to correct the incorrectly labeled data on the dev set. Which of these statements do you agree with? (Check all that apply).\n",
    "\n",
    "-[x] You should also correct the incorrectly labeled data in the test set, so that the dev and test sets continue to come from the same distribution\n",
    "\n",
    "-[] You should correct incorrectly labeled data in the training set as well so as to avoid your training set now being even more different from your dev set.\n",
    "\n",
    "-[] You should not correct the incorrectly labeled data in the test set, so that the dev and test sets continue to come from the same distribution\n",
    "\n",
    "-[x] You should not correct incorrectly labeled data in the training set as well so as to avoid your training set now being even more different from your dev set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far your algorithm only recognizes red and green traffic lights. One of your colleagues in the startup is starting to work on recognizing a yellow traffic light. (Some countries call it an orange light rather than a yellow light; we’ll use the US convention of calling it yellow.) Images containing yellow lights are quite rare, and she doesn’t have enough data to build a good model. She hopes you can help her out using transfer learning.\n",
    "\n",
    "What do you tell your colleague?\n",
    "\n",
    "-[x] She should try using weights pre-trained on your dataset, and fine-tuning further with the yellow-light dataset.\n",
    "\n",
    "-[] If she has (say) 10,000 images of yellow lights, randomly sample 10,000 images from your dataset and put your and her data together. This prevents your dataset from “swamping” the yellow lights dataset.\n",
    "\n",
    "-[] You cannot help her because the distribution of data you have is different from hers, and is also lacking the yellow label.\n",
    "\n",
    "-[] Recommend that she try multi-task learning instead of transfer learning using all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. New task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another colleague wants to use microphones placed outside the car to better hear if there’re other vehicles around you. For example, if there is a police vehicle behind you, you would be able to hear their siren. However, they don’t have much to train this audio system. How can you help?\n",
    "\n",
    "-[] Transfer learning from your vision dataset could help your colleague get going faster. Multi-task learning seems significantly less promising.\n",
    "\n",
    "-[] Multi-task learning from your vision dataset could help your colleague get going faster. Transfer learning seems significantly less promising.\n",
    "\n",
    "-[] Either transfer learning or multi-task learning could help our colleague get going faster.\n",
    "\n",
    "-[x] Neither transfer learning nor multi-task learning seems promising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. End-to-end approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recognize red and green lights, you have been using this approach:\n",
    "\n",
    "(A) Input an image (x) to a neural network and have it directly learn a mapping to make a prediction as to whether there’s a red light and/or green light (y).\n",
    "A teammate proposes a different, two-step approach:\n",
    "\n",
    "(B) In this two-step approach, you would first (i) detect the traffic light in the image (if any), then (ii) determine the color of the illuminated lamp in the traffic light.\n",
    "Between these two, Approach B is more of an end-to-end approach because it has distinct steps for the input end and the output end. True/False?\n",
    "\n",
    "-[] True\n",
    "\n",
    "-[x] False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. End-to-end approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach A (in the question above) tends to be more promising than approach B if you have a ________ (fill in the blank).\n",
    "-[x] Large training set\n",
    "\n",
    "-[] Multi-task learning problem.\n",
    "\n",
    "-[] Large bias problem.\n",
    "\n",
    "-[] Problem with a high Bayes error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
